{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yuliya Akchurina\n",
    "# CISC6210 - Natural Language Processing \n",
    "# Homework2 - Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import copy \n",
    "\n",
    "url = \"https://storm.cis.fordham.edu/~yli/data/MyShakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_preprocess(url):\n",
    "    # Uploading and preprocessing text. Tokenise words. Keep the punctuation and special characters.\n",
    "    #import text from a url. Returns string variable with all text in it. String lenght 51896\n",
    "    raw = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "    #convert to lower case. Returns string variable \n",
    "    raw_lower = raw.lower()\n",
    "\n",
    "    # split text into separte words. Returns list of lowercase word tokens, all punctuation included. 11320 tokens with punctuation.  \n",
    "    word_tokens = nltk.word_tokenize(raw_lower)\n",
    "    return word_tokens\n",
    "\n",
    "def get_vocab(word_tokens):\n",
    "    vocab_list = list(set(word_tokens))\n",
    "    return vocab_list\n",
    "\n",
    "def size_vocab(word_tokens):\n",
    "    vocab_size = len(list(set(word_tokens)))\n",
    "    return vocab_size \n",
    "\n",
    "def len_text(word_tokens):\n",
    "    length_text = len(word_tokens)\n",
    "    return length_text\n",
    "\n",
    "# Obtain list of Ngrams. Takes in n - number of n in ngrams, and text - list of toknized words\n",
    "def get_ngram(text, n):\n",
    "    ngram_list = list(ngrams(text, n))\n",
    "    return ngram_list\n",
    "\n",
    "def len_ngram(ngram_list):\n",
    "    return len(ngram_list)\n",
    "\n",
    "def ngram_freq(gram):\n",
    "    #returns frequency distribution of ngrams and a tuple for the most frequent ngram \n",
    "    fdist = FreqDist(gram) \n",
    "    return fdist\n",
    "\n",
    "def input_ngram(context, word):\n",
    "    \n",
    "    context = input(f\"Please enter a beginning of Ngram string length {n-1} :   \")\n",
    "    word = input(\"Please enter the last word of the NGram:   \")\n",
    "    \n",
    "    context = context.lower()\n",
    "    word = word.lower()\n",
    "    \n",
    "    full_ngram = context + word\n",
    "    \n",
    "    #context_token = nltk.word_tokenize(context)\n",
    "    #last_word_token = nltk.word_tokenize(word)\n",
    "    \n",
    "    #full_ngram = copy.deepcopy(context_token)\n",
    "    #full_ngram.extend(last_word_token)\n",
    "    return context, word, full_ngram\n",
    "\n",
    "\n",
    "def process_text(context, word):\n",
    "    #takes in strings, returns tokenized lists \n",
    "    context = context.lower()\n",
    "    word = word.lower()\n",
    "\n",
    "    context_token = nltk.word_tokenize(context)\n",
    "    last_word_token = nltk.word_tokenize(word)\n",
    "    return context_token, last_word_token\n",
    "\n",
    "\n",
    "def get_prob(context, word, n):\n",
    "    # takes in strings, returns probabiity\n",
    "    \"\"\" Prob(word | context) = (count(context+word)) / count(context)) \"\"\"\n",
    "    \n",
    "    if n == 1:\n",
    "        probability = 1/vocab_size\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # create context ngrams, n-1 ngrams and word ngrams from given text url\n",
    "        context_gram = get_ngram(upload_preprocess(url), n-1)\n",
    "        freqdist_context_gram = ngram_freq(context_gram)\n",
    "        \n",
    "        word_gram = get_ngram(upload_preprocess(url), 1)\n",
    "        freqdist_word_gram = ngram_freq(word_gram)     \n",
    "\n",
    "        n_gram = get_ngram(upload_preprocess(url), n)\n",
    "        freqdist_n_gram = ngram_freq(n_gram)\n",
    "        \n",
    "        context_token, last_word_token = process_text(context, word)\n",
    "        \n",
    "        # check if context and word occur in text \n",
    "        context_tuple = tuple(context_token)\n",
    "        word_tuple = tuple(last_word_token)\n",
    "        full_ngram = []\n",
    "        full_ngram = context_token + last_word_token\n",
    "        full_ngram_tuple = tuple(full_ngram)\n",
    "\n",
    "        # Calculate counts and probabilities \n",
    "        count_ngram = freqdist_n_gram.get(full_ngram_tuple)\n",
    "        count_context = freqdist_context_gram.get(context_tuple)\n",
    "\n",
    "        # check context  \n",
    "        if context_tuple in freqdist_context_gram.keys():\n",
    "            print(\"Context exists\")\n",
    "            #print(freqdist_context_gram.get(context_tuple))\n",
    "            context_seen = True\n",
    "        else:\n",
    "            print(\"Context not found\")\n",
    "            context_seen = False\n",
    "\n",
    "        # check word\n",
    "        if word_tuple in freqdist_word_gram.keys():\n",
    "            print(\"Word exists\")\n",
    "            #print(freqdist_word_gram.get(word_tuple))\n",
    "            word_seen = True\n",
    "        else:\n",
    "            print(\"Word not found\")\n",
    "            probability_word = 1/(size_vocab(upload_preprocess(url)) + 1)\n",
    "            word_seen = False\n",
    "        if full_ngram_tuple in freqdist_n_gram.keys():\n",
    "            print(\"Ngram exists\")\n",
    "            #print(freqdist_word_gram.get(word_tuple))\n",
    "            ngram_seen = True\n",
    "        else:\n",
    "            print(\"Ngram not found\")\n",
    "            ngram_seen = False\n",
    "\n",
    "        #if context_seen and word_seen:\n",
    "            #probability = count_ngram/count_context\n",
    "            \n",
    "        if context_seen and word_seen and ngram_seen:\n",
    "            probability = count_ngram/count_context\n",
    "\n",
    "        elif context_seen and word_seen and not ngram_seen:\n",
    "            # if the context and word exist in the text but the ngram of context+word does not exist in the text. \n",
    "            count_ngram = 0 + 1\n",
    "            probability = count_ngram/count_context\n",
    "\n",
    "        elif context_seen and not word_seen:\n",
    "            print(\"The context is the text. The word is not in the text.\")\n",
    "            count_ngram = 0\n",
    "            probability = count_ngram/count_context\n",
    "\n",
    "        elif not context_seen and word_seen:\n",
    "            #If you encounter a novel context, and the word exists in the vocabulary, \n",
    "            #the probability of any given word should be 1/the size of the vocab.\n",
    "            print(\"The context is not in the text. The word is in the text.\")\n",
    "            probability = 1/size_vocab(upload_preprocess(url))\n",
    "\n",
    "        elif not context_seen and not word_seen:\n",
    "            #If the word is unknown, the probability of the unknown word should be 1/(the size of vocab+1)\n",
    "            print(\"The context is not in the text. The word is not in the text.\")\n",
    "            probability = 1/(size_vocab(upload_preprocess(url)) + 1)\n",
    "\n",
    "        else: \n",
    "            print(\" Oops! Something went wrong when calculating probabilities. \")\n",
    "\n",
    "    print(f\"Probability of the word: '{word}' given context: '{context}' is = {probability}\")\n",
    "\n",
    "    return probability\n",
    "\n",
    "\n",
    "def generate_text(context, max_length):\n",
    "    context_lower = context.lower()      #convert to lowercase\n",
    "    context_token = nltk.word_tokenize(context_lower)   #tokenize \n",
    "    k = len(context_token)   #context token count\n",
    "\n",
    "    # get all k+1 size ngrams\n",
    "    plusOne_ngram = get_ngram(upload_preprocess(url), k+1)\n",
    "    freqdist_plusOne_ngram = ngram_freq(plusOne_ngram) \n",
    "    \n",
    "    # nested list of length 6320\n",
    "    dictlist = list(freqdist_plusOne_ngram.items())\n",
    "    \n",
    "    ngram_start_list = []\n",
    "    frequency_sent_list = []\n",
    "    \n",
    "    search_token = copy.deepcopy(context_token)\n",
    "    result_sentence = copy.deepcopy(context_token)\n",
    "    \n",
    "    while len(result_sentence) < max_length:  \n",
    "        max_match_ngram = []\n",
    "        ngram_start_list =[]\n",
    "        \n",
    "        for i in range(len(dictlist)):\n",
    "            if (' '.join(dictlist[i][0])).startswith(' '.join(search_token)):\n",
    "                #print(dictlist[i])\n",
    "                ngram_start_list.append(dictlist[i])\n",
    "\n",
    "        # update the context sentence with new word until the length of the sentense reached max_length\n",
    "        max_match_ngram = list(max(ngram_start_list, key=lambda x: x[1])[0])\n",
    "        #print(f\"max_match_ngram {max_match_ngram}\")\n",
    "        \n",
    "        result_sentence.append(max_match_ngram[len(max_match_ngram)-1])\n",
    "        frequency_sent_list.append(max(ngram_start_list, key=lambda x: x[1])[1])   #frequency\n",
    "        \n",
    "        #update search token\n",
    "        l = len(result_sentence)\n",
    "        search_token = []\n",
    "        search_token.extend([result_sentence[l-2],result_sentence[l-1]])\n",
    "    \n",
    "    return(result_sentence)\n",
    "    #return(result_sentence, frequency_sent_list)\n",
    "\n",
    "def perplexity(text, n):\n",
    "    text_token = nltk.word_tokenize(text)\n",
    "    text_len = (len(text_token))\n",
    "    prob_list = []\n",
    "\n",
    "    for i in range(0, text_len - 2):\n",
    "        calculated_prob = get_prob([text_token[i], text_token[i+1]], [text_token[i+2]], n)\n",
    "        prob_list.append(calculated_prob)\n",
    "\n",
    "    product = 1\n",
    "    for item in prob_list:\n",
    "        product = product * item\n",
    "        get_perplexity = (product)**(-1/text_len)\n",
    "    return get_perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all functions \n",
    "n = 3\n",
    "\n",
    "word_tokenize = upload_preprocess(url)\n",
    "vocab_list = get_vocab(word_tokenize)\n",
    "vocab_size = size_vocab(word_tokenize)\n",
    "text_length = len_text(word_tokenize)\n",
    "ngram_list = get_ngram(word_tokenize, n)\n",
    "ngram_list_len = len_ngram(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context exists\n",
      "Word exists\n",
      "Ngram exists\n",
      "Probability of the word: 'is' given context: 'our business' is = 1.0\n",
      "1.0\n",
      "Context exists\n",
      "Word exists\n",
      "Ngram exists\n",
      "Probability of the word: 'business' given context: 'our' is = 0.03225806451612903\n",
      "0.03225806451612903\n",
      "Probability of the word: 'business' given context: ' ' is = 0.0006978367062107466\n",
      "0.0006978367062107466\n"
     ]
    }
   ],
   "source": [
    "# testing Pobability function\n",
    "\n",
    "wordone = \"our business\"\n",
    "wordtwo = \"is\"\n",
    "prob_test = get_prob(wordone, wordtwo, n=3)\n",
    "\n",
    "print(prob_test)\n",
    "\n",
    "wordone = \"our\"\n",
    "wordtwo = \"business\"\n",
    "prob_test2 = get_prob(wordone, wordtwo, n=2)\n",
    "\n",
    "print(prob_test2)\n",
    "\n",
    "wordone = \" \"\n",
    "wordtwo = \"business\"\n",
    "prob_test3 = get_prob(wordone, wordtwo, n=1)\n",
    "\n",
    "print(prob_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30 word generated sentence from three-gram model is: \n",
      "\n",
      "['our', 'business', 'is', 'not', 'unknown', 'to', 'the', 'capitol', ';', 'who', \"'s\", 'like', 'to', 'rise', ',', 'who', 'is', 'the', 'sink', 'o', \"'\", 'the', 'body', ',', 'idle', 'and', 'unactive', ',', 'still', 'cupboarding']\n",
      "\n",
      "Length of the sentence 30\n"
     ]
    }
   ],
   "source": [
    "### Generate a sentence length 30 with seed words \"our business\" based on the 3gram modeL ###\n",
    "context_seed = \"our business\"\n",
    "generated_sentence = generate_text(context_seed, 30)\n",
    "print(f\"The 30 word generated sentence from three-gram model is: \\n\\n{generated_sentence}\")\n",
    "print(f\"\\nLength of the sentence {len(generated_sentence)}\")\n",
    "\n",
    "\"\"\"['our', 'business', 'is', 'not', 'unknown', 'to', 'the', 'capitol', ';', 'who', \n",
    "\"'s\", 'like','to', 'rise', ',', 'who', 'is', 'the', 'sink', 'o', \n",
    "\"'\", 'the', 'body', ',', 'idle', 'and', 'unactive',',', 'still', 'cupboarding']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = get_ngram(word_tokenize, n=1)     #11320\n",
    "bigrams = get_ngram(word_tokenize, n=2)    #11319\n",
    "trigrams = get_ngram(word_tokenize, n=3)    #11318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',',), 842), ((':',), 506), (('.',), 397), (('the',), 369), (('you',), 219), (('and',), 192), (('i',), 187), (('to',), 178), ((';',), 160), (('a',), 117)]\n",
      "[((',', 'and'), 71), (('citizen', ':'), 58), ((',', 'i'), 53), (('marcius', ':'), 53), (('first', 'citizen'), 46), ((',', 'you'), 39), (('.', 'marcius'), 37), (('.', 'first'), 36), ((',', 'the'), 36), (('menenius', ':'), 36)]\n",
      "[(('first', 'citizen', ':'), 46), (('.', 'marcius', ':'), 37), (('.', 'virgilia', ':'), 28), (('.', 'first', 'citizen'), 26), (('.', 'valeria', ':'), 24), (('.', 'menenius', ':'), 20), (('o', \"'\", 'the'), 18), (('.', 'lartius', ':'), 16), (('first', 'senator', ':'), 15), (('?', 'first', 'citizen'), 14)]\n"
     ]
    }
   ],
   "source": [
    "freqdist_unigram = ngram_freq(unigrams)\n",
    "freqdist_bigram = ngram_freq(bigrams)\n",
    "freqdist_trigram = ngram_freq(trigrams)\n",
    "print(freqdist_unigram.most_common(10))\n",
    "print(freqdist_bigram.most_common(10))\n",
    "print(freqdist_trigram.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Perplexity = 16.261963246864724\n"
     ]
    }
   ],
   "source": [
    "# Perplexity for trigrams \n",
    "# n=3 \n",
    "text = \"make you a sword for me\" \n",
    "text_token = nltk.word_tokenize(text.lower())\n",
    "\n",
    "probability_list = []\n",
    "\n",
    "for i in range(0, int(len(text_token)-2)):\n",
    "    context_tuple = tuple([text_token[i], text_token[i+1]])\n",
    "    #word_tuple = tuple(text_token[i+2])\n",
    " \n",
    "    trigram_tuple = tuple([text_token[i], text_token[i+1], text_token[i+2]])\n",
    "\n",
    "    # Calculate counts and probabilities \n",
    "    count_ngram = freqdist_trigram.get(trigram_tuple)\n",
    "    count_context = freqdist_bigram.get(context_tuple)\n",
    "    \n",
    "    if count_context is not None and count_ngram is not None:\n",
    "        probability = count_ngram/count_context      \n",
    "    \n",
    "    elif count_context is None:\n",
    "        probability = 1/vocab_size\n",
    "           \n",
    "    elif count_ngram is None:\n",
    "        probability = 1/(vocab_size+1)\n",
    "      \n",
    "    probability_list.append(probability)\n",
    "\n",
    "product = 1\n",
    "for item in probability_list:\n",
    "    product = product*item\n",
    "\n",
    "    \n",
    "trigram_perplexity = product**(-1/len(text_token))\n",
    "print(f\"Trigram Perplexity = {trigram_perplexity}\")  # Trigram Perplexity 16.261963246864724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity = 426.78108055853585\n"
     ]
    }
   ],
   "source": [
    "# Perplexity for bigrams \n",
    "# n=2 \n",
    "text = \"make you a sword for me\" \n",
    "text_token = nltk.word_tokenize(text.lower())\n",
    "\n",
    "probability_list = []\n",
    "\n",
    "for i in range(0, int(len(text_token)-1)):\n",
    "    #context_tuple = tuple(text_token[i])\n",
    "    #word_tuple = tuple(text_token[i+1])\n",
    "    context = text_token[i]\n",
    "    word = text_token[i+1]\n",
    "\n",
    "    bigram_tuple = tuple([text_token[i], text_token[i+1]])\n",
    "\n",
    "    # Calculate counts and probabilities \n",
    "    count_ngram = freqdist_bigram.get(bigram_tuple)\n",
    "    count_context = freqdist_unigram.get(context)\n",
    "    \n",
    "    if count_context is not None and count_ngram is not None:\n",
    "        probability = count_ngram/count_context\n",
    "        \n",
    "    \n",
    "    elif count_context is None:\n",
    "        probability = 1/vocab_size\n",
    "        \n",
    "    \n",
    "    elif count_ngram is None:\n",
    "        probability = 1/(vocab_size+1)\n",
    "      \n",
    "    probability_list.append(probability)\n",
    "    \n",
    "#print(probability_list)\n",
    "\n",
    "product = 1\n",
    "for item in probability_list:\n",
    "    product = product*item\n",
    "\n",
    "bigram_perplexity = product**(-1/len(text_token))\n",
    "print(f\"Bigram Perplexity = {bigram_perplexity}\")   # Bigram Perplexity 426.78108055853585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity = 1433.0 \n"
     ]
    }
   ],
   "source": [
    "# Perplexity for unigrams \n",
    "# n=1\n",
    "text = \"make you a sword for me\"    \n",
    "text = nltk.word_tokenize(context.lower())\n",
    "\n",
    "text_len = (len(text))\n",
    "prob_list = []\n",
    "n=1\n",
    "\n",
    "for word in text:\n",
    "    calculated_prob = 1/vocab_size\n",
    "    prob_list.append(calculated_prob)\n",
    "\n",
    "product = 1\n",
    "for item in prob_list:\n",
    "    product = product * item\n",
    "unigram_perplexity = (product)**(-1/text_len)\n",
    "\n",
    "print(f\"Unigram perplexity = {unigram_perplexity} \")  # Unigram perplexity 1432.9999999999995 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
